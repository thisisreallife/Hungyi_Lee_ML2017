## 信息熵的定义：

假设有随机变量$X$，第$i$ 类样本所占比例为$p_i$,已知$\sum_{1}^{n}p_i = 1$。信息熵的定义为：
$$
\mathrm{Ent(X)} = -\sum_{1}^{n}p_i \log_2p_i
$$
求证：$0 \leq \mathrm {Ent(X)} \leq n\log n$

### 证明：

首先，因为$p_i \log p_i <= 0$，所以$0 \leq \mathrm{Ent(X)} $。
构造拉格朗日函数并令偏导数为0:
$$
\begin{align}
L(p_i,\lambda) &=-\sum_{1}^{n}p_i \log_2p_i + \lambda(\sum_{i}^{n}p_i -1) \\
\frac{\partial L}{\partial p_i} &= -\log_2p_i - \frac{1}{\log2} + \lambda = 0\\
\frac{\partial L}{\partial \lambda} &= \sum_{1}^{n}p_i - 1 = 0
\end{align}
$$
求解得到：
$$
\begin{align}
p_i &= 2^{\lambda - \frac{1}{\ln 2}} \\
\lambda &= -\log_2 n - \frac{1}{\ln 2}
\end{align} 
$$
当$p_i$ 和$\lambda$取值如上的时候，信息熵取最大值$n \log n$



条件熵：

设有随机变量(X,Y)，其联合概率分布为$P\left(X=x_{i}, Y=y_{j}\right)=p_{i j}, i=1 \ldots n, j=1 \ldots m$。条件熵$\mathrm{H}(\mathrm{Y} | \mathrm{X})$表示在已知随机变量X的条件下随机变量Y的不确定性。随机变量X给定的条件下随机变量Y的条件熵(conditional entropy)H(Y|X)，定义为X给定条件下Y的条件概率分布的熵对X的数学期望。
$$
H(Y | X)=\sum_{I=1}^{m} p_{i} H\left(Y | X=x_{i}\right)
$$
其中$p_{i}=P\left(X=x_{i}\right), \quad i=1,2, . . n$。

### 条件熵的推导

条件熵指的是（X,Y）联合熵减去X的信息熵。
$$
\begin{align}
& \ \ \ \ \  H(X, Y)-H(X) \\ 
&= -\sum_{x, y} P(x, y) \log (x, y)+\sum_{x} P(x) \log P(x) \\ 
&= -\sum_{x, y} P(x, y) \log (x, y)+\sum_{x}\left(\sum_{y} P(x, y)\right) \log P(x) \\
&= -\sum_{x, y} P(x, y) \log (x, y)+\sum_{x, y} P(x, y) \log P(x) \\
&= -\sum_{x, y} P(x, y) \log \frac{P(x, y)}{P(x)} \\ 
&= -\sum_{x, y} P(x, y) \log P(y | x) \\
& = -\sum_{x}P(x)(\sum_{y}P(y|x)logP(y | x))\\
& = -\sum_{x}P(x)H(Y|X = x)
\end{align}
$$


## 联合熵：

对服从联合分布为$\mathrm{P}(\mathrm{x}, \mathrm{y})$的一对离散随机变量$(X, Y)$，其联合熵$\mathrm{H}(\mathrm{X}, \mathrm{Y})$可以表示为
$$
H(X, Y)=-\sum_{x \in X} \sum_{y \in Y} P(x, y) \log P(x, y)
$$


## 边际概率：

当X和Y都是离散随机变量时，X和Y的联合概率分布记为$\mathrm{P}(\mathrm{x}, \mathrm{y})$。

X的分布列为：
$$

P_{X}(x)=P(X=x)=\sum_{y} P(x, y)
$$
Y的分布列记为：
$$
P_{Y}(y)=P(Y=y)=\sum_{x} P(x, y)
$$


