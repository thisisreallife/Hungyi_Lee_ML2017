### 理解偏差和方差

bias描述的是根据样本拟合出的模型输出预测结果的期望与样本真实值得差距。简单来说：就是样本拟合的好不好

想在bias上表现良好，low bias，就得复杂化模型。复杂化模型就会导致过拟合(overfitting)现象的发生

variance描述的是样本上训练出来的模型在测试集上的表现，我们一直想要的是在训练集上表现良好。

想要在variance表现良好，就得简化模型，减少模型的参数。但是这样的话就会导致欠拟合(unfitting)现象


![image](https://note.youdao.com/yws/res/54134/82B7AB600215418E90F0062B4BFEE467)
![image](https://note.youdao.com/yws/res/54136/9FA5087EE05C45A4872746885372851B)

### 误差的推导
对于观测数据X以及待测的变量Y，两者服从`$Y=f(x)+\sigma$`，`$\sigma$`是误差，其服从于正态分布N(0,`$\varepsilon^2$`)。首先在数据集D上通过算法学习一个近似f(x)的模型`$\hat{f(x)} $`来预测x的输出。

##### E(X)是随机变量X的所有可能取值的加权平均

1对应着是第三幅图；2对应第四幅图；3对应着第二幅图；4对应着第一幅图，第一幅图也是我们最希望的。

四幅图是所说的欠拟合，没有学到东西；第三幅图是过拟合，我都学到旁边去了，中心目标没有命中。第二幅图是我们一般勉为其难选的一个模型。也一部分命中了，有一部分没有命中。

#### 方差的定义
方差=平方的期望减期望的方差

```math
var=E[(X-\mu)^2]=E[X^2-2X\mu+\mu^2]=E(X^2)-2\mu^2+\mu^2 = E(X^2)-\mu^2
```


```math
E[X^2]=Var[X]+(E[X])^2
```

#### 测试样本y期望

```math
E[f]=f

y=f+\varepsilon

E[\varepsilon]=0

var[\varepsilon]=\sigma^2

```


```math
E[y]=E[f+\varepsilon]=f
```
#### 测试样本的方差

```math
var[y]=E[(y-E[y])^2]

var[y]=E[(y-f)^2]

var[y]=(f+\varepsilon -f)^2

var[y]=E[\varepsilon ^2]

var[y]=Var[\varepsilon ]+(E[\varepsilon ]^2)=\varepsilon ^2
```

### 详细的推导如下
检测预测值型`$\hat{f(x)} $`与实际值y之间的误差，可以采用平方误差函数(mean squared erroe)来模拟的好坏程度，即


```math
Error(X)=(y-\hat{f(x)} )^2

E[Error(X)]=E[(y-\hat{f(x)} )^2]
```
![image](https://note.youdao.com/yws/res/54213/42E0243A3017478C89E861ECF0ECC77B)

### 全局最优与局部最优
![image](https://note.youdao.com/yws/res/54143/C9AA11C22D394ADDB12FC8271A4294DF)

### 鞍点
鞍点(saddle)是函数上的导数为零，但不是轴上局部极值的点，通常梯度为零的点是上图所示的鞍点，而非局部最小值。减少损失的难度也来自于误差曲面中的鞍点，而不是全局最优点 

![image](https://note.youdao.com/yws/res/54146/5D5B9D7BE7C74B0697E2C123F05851BA)
- 在训练较大的神经网络、存在大量参数，并且成本函数被定义在较高的维度空间时，困在极差的局部最优基本不会发生

- 鞍点附近的平稳段会非常缓慢，而这也是需要后面的动量梯度下降法，RMSProp以及Adam优化算法能够加速学习的原因，它们能帮助今早走出平稳段

### 解决方法：
#### Mini-Batch Gradient Descent
- Mini-Batch Gradient Descent(小批量梯度下降)，每次同时处理固定大小的数据集
#### Stochastic gradient descent
- Mini-batch的大小为1，即是随机梯度下降



![image](https://note.youdao.com/yws/res/54171/1CCF3B2F529C474C846FA45D920B3024)


### 批量梯度下降与Mini-batch梯度下降的区别
batch梯度下降和Mini-batch梯度下降法代价函数的变化趋势如下
![image](https://note.youdao.com/yws/res/54171/1CCF3B2F529C474C846FA45D920B3024)


#### 梯度下降优化影响
- batch梯度下降
- - 对所有m个训练样本执行一次梯度下降，每一次迭代时间比较长，训练过程慢
- - 相对噪声低一些，损失函数总是向减小的方向下降
- 随机梯度下降法(Mini-batct=1)对每一个样执行一次梯度下降，训练速度快，但丢失了向量化带来的计算加速
- - 有很多的噪声，需要适当减少学习率。损失函数总体趋势向全局最小值靠近，但永远不会收敛，而是一直在最小是附近波动
- - Mini-batch梯度下降，可以实现快速学习，也可以应用向量化带来的好处，且损失函数的下降处于前两者之间。

### 大小选择
- 如果训练样本的大小，比如`$m\leqslant200 $`时，选择batch梯度下降法
- 如果训练样本的大小比较大，选择Mini-batch梯度下降法，为了和计算机的信息存储方式相适应，代码能在mini-batch大小为2的幂次时运行要快一些。典型的大小为`$2^6(64),2^7(128),2^8(256),2^9(512)$`mini-batch的大小要符合GPU/CPU内存



### 特征归一化
以下说明来自《百面机器学习》中：第1章特征工程：第一节特征归一化

==为了消除数据特征之间的量纲影响==，我们需要对特征记性归一化处理，==使得不同指标之间具有可比==性。

例如，分析一个人的身高和体重对健康的影响，如果使用米(m)和千克(kg)作为单位，那么身高特征会在1.6-1.8的数值范围内，体重特征会在50-100kg的范围内，分析出来的结果显然会倾向于比较大的体重特征。想要得到更为准确的结果，==就需要进行归一化==(Normalization)处理，使各指标处于同一数值量级，以便进行分析

对数值类型的特征做归一化可以将所有的特征都同一到一个大致相同的数值区间内。最常用的方法主要有以下两种

- 线性归一化(Min-Max Scaling)。它对原始数进行线性变化，使结果映射到[0,1]的范围，实现对原始数据的等比缩放。归一化公式如下，其中X为原始数据，Xmax、Xmin分别为数据最大值和最小值。

```math
X_{norm} = \frac{X-X_{min}}{X_{max}-X_{min}}
```
- 代码详见Task.py(参考代码)

- 零均值归一化(Z-Score Normalization)，它会将原始数据映射到均值为0，标准差为1的分布上。具体来说，假设原始特征的均值为μ、标准差为σ，那么
归一化公式定义为

```math
z=\frac{x-\mu }{\sigma }
```
### 为什么需要对数值型特征做归一化呢？
我们不妨借助随机梯度下降的实例来
说明归一化的重要性。假设有两种数值型特征，x1的取值范围为 [0, 10]，x2的取值
范围为[0, 3]，于是可以构造一个目标函数符合图1.1（a）中的等值图

在学习速率相同的情况下，x1的更新速度会大于x2，需要较多的迭代才能找到
最优解。如果将x1和x2归一化到相同的数值区间后，优化目标的等值图会变成图
1.1（b）中的圆形，x1和x2的更新速度变得更为一致，容易更快地通过梯度下降找
到最优解。


![image](https://note.youdao.com/yws/res/54278/7436925014144AF7AEF2D82350A0904E)

当然，数据归一化并不是万能的。在实际应用中，通过梯度下降法求解的模
型通常是需要归一化的，包括线性回归、逻辑回归、支持向量机、神经网络等模
型。但对于决策树模型则并不适用，以C4.5为例，决策树在进行节点分裂时主要
依据数据集D关于特征x的信息增益比（详见第3章第3节），而信息增益比跟特征
是否经过归一化是无关的，因为归一化并不会改变样本在特征x上的信息增益。

### 线性回归模型评估
当我们建立好模型后，模型的效果如何？我们可以采用如下的指标来进行衡量

- MSE
- RMSE
- MAE
- `$R^2$`


#### MSE
MSE(mean squared Error)，平均平方误差，为所有样本误差(真实值与预测值之差)的平方和，然后取均值

```math
MSE=\frac{1}{m}\sum_{i=1}^{m}(y^i-\hat{y}^i)^2
```
#### RMSE
RMSE(Root mean squared Error)，平均平方误差的平方根，即在MSE的基础上，取平方根

```math
MSE=\sqrt{\frac{1}{m}\sum_{i=1}^{m}(y^i-\hat{y}^i)^2}
```

### MAE
MAE（Mean Absolute Error），平均绝对值误差，为所有样本误差的绝对值和。  


```math
MAE = \frac{1}{m}\sum_{i=1}^{m}|y^{(i)}-\hat{y}^{(i)}|
```

### `$R^2$`

`$R ^ {2}$`为决定系数，用来表示模型拟合性的分值，值越高表示模型拟合性越好，最高为1，可能为负值。  
`$R ^ {2}$`的计算公式为1减去RSS与TSS的商。其中，TSS（Total Sum of Squares）为所有样本与均值的差异，是方差的m倍。而RSS（Residual sum of squares）为所有样本误差的平方和，是MSE的m倍。  


```math
R^{2}=1-\frac{RSS}{TSS} =1-\frac{\sum_{i=1}^{m}(y^i-\hat{y}^i)^2}{\sum_{i=1}^{m}(y^i)-\bar{y}^2}

\bar{y}=\frac{1}{m}\sum_{i=1}^{m}y^i
```






