**负责人：王佳旭**
**课程设计人：王佳旭**

**【线性回归任务一】**

**#任务时间#**
请于5月13日22:00前完成，逾期尚未打卡的会被清退。

**学习视频内容：**
* 观看李宏毅课程内容：P1、P2。
  * 视频连接：[https://www.bilibili.com/video/av35932863?from=search&seid=2134843831238226258](https://www.bilibili.com/video/av35932863?from=search&seid=2134843831238226258)

**学习打卡任务内容：**
* 了解什么是Machine learning
* 学习中心极限定理，学习正态分布，学习最大似然估计
  * 推导回归Loss function
  * 学习损失函数与凸函数之间的关系
  * 了解全局最优和局部最优
* 学习导数，泰勒展开
  * 推导梯度下降公式
  * 写出梯度下降的代码
* 学习L2-Norm，L1-Norm，L0-Norm
  * 推导正则化公式
  * 说明为什么用L1-Norm代替L0-Norm
  * 学习为什么只对w/Θ做限制，不对b做限制

**学习任务说明：**
1. 建立CSDN账号或者简书等开源账号，将学习打卡任务写到CSDN或者简单等开源平台上
2. 未按时打卡这将会被清退
3. 打卡地点：李宏毅机器学习第七群
4. 打卡形式：

Datawhale第7期-《李宏毅机器学习》作业统计 - Task1（请按顺序接龙）

作业链接：
1.王佳旭(昵称)——[www.baidu.com](http://www.baidu.com)      [(完成打卡学习链接](http://www.baidu.com(完成学习链接)[)](http://www.CSDN())
2.
3.
4.
...

**参考内容：**
李宏毅机器学习课程
Datawhale整理开源笔记《李宏毅机器学习》(现在未完成，只能先放出一点点，还请小伙伴多给点时间)

**参考链接：**
参考手写公式链接：在学习第二天我会公布(5.12)
参考代码链接：在学习第二天我会公布（5.12）

在第二天给出数学参考是因为：更多想把时间留给学习者，我所写的只是参考，希望大家有自己的想法

**参考链接代码和数学：**

[参考数学.md](https://uploader.shimo.im/f/ND4AqiGGqlQTCEFy.md)

[Batch Gradient Descent.ipynb](https://uploader.shimo.im/f/UvVLrlY6xg86VhZ9.ipynb)


Datawhale开源笔记链接《李宏毅机器学习》：
[https://datawhalechina.github.io/Leeml-Book/#/chapter2/chapter2](https://datawhalechina.github.io/Leeml-Book/#/chapter2/chapter2)
